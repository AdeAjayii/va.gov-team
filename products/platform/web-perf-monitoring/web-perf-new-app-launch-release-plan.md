# Website Performance Monitoring for New Application Launches

---
## Phase I: Moderated Testing

#### Plan: VSF Team ready to launch to production
### Planning:
- Desired date range or test duration: 
- Desired number of users: 1 Team
- How you'll conduct the testing: Provide a team with access to the Speedcurve and our documentation. Have them document performance tests run that demonstrate that their application meets or exceeds performance standards.
- How you'll give the test users access to the product in production w/o making it live on VA.gov: Testing  can be performed with an unpublished url or using a staging environment

### Results:
- Number of users:  
- Number of bugs identified / fixed: 
- Was the data submitted (if any) easy for VA to process?
- Types of errors logged: 
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? 
- If yes, what: 

## Phase II: Unmoderated testing

#### Plan: Future VFS Team Launching to Production
### Planning:
- Desired date range: 
- Desired number of unique users: 
- How you'll make the product available in production while limiting the # of users who can find/access it: 
- "Success" criteria (by the numbers):

### Results:
- Number of unique users: 2 Users
- Actual results

- Was the data submitted (if any) easy for VA to process?:
- Types of errors logged: 
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges?
- If yes, what: N/A


## Go Live!

### Planning:
- Desired date: ?
- Post-launch KPI 1: 100% of Teams Launching an application pass performance testing
- Post-launch KPI 2: 
- Post-launch KPI 3:
- Post-launch KPI 4: 

#### Create a full list of teams on VSP
- [ ] Identify team admins (product leads, dev leads, FE devs)
  
#### Create teams on speedcurve
- [ ] Add Admins to their teams
- [ ] Create performance budget dashboards
- [ ] Link perf budget alerts to Slack
  
#### Demo of Speedcurve Functionality
  - [ ] Schedule & Record Zoom Meeting Demo
  - [ ] Link to Zoom Recording in Help Documents

#### Speedcurve Documentation
  - [ ] 'VSP Setting up Speedcurve' Documentation
  - [ ] 'VFS Team Admin Options for Speedcurve' Documentation
  - [ ] 'Track, Test, Optimize Performance with Speedcurve'
 

- Go / No Go: (ready / not ready)[https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/platform/product-management/go-no-go-meeting-template.md]

### 1-week results:
- Number of unique users: x
- Post-launch KPI 1: VSP has setup 100% of VFS teams in Speedcurve
- Post-launch KPI 2: Number of metrics that exceed performance budgets
- Post-launch KPI 3: Performance gains per team per metric
- Post-launch KPI 4: Number of optimization opportunities identified and scoped
- Any issues with VA handling/processing?: yes/no, lorem ipsum
- Types of errors logged: lorem ipsum
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? yes/no 
- If yes, what: lorem ipsum

### 1-month results:
- Number of unique users: x
- Post-launch KPI 1: VSP has setup 100% of VFS teams in Speedcurve
- Post-launch KPI 2: Number of metrics that exceed performance budgets
- Post-launch KPI 3: Performance gains per team per metric
- Post-launch KPI 4: Number of optimization opportunities identified and scoped
- Any issues with VA handling/processing?: yes/no, lorem ipsum
- Types of errors logged: lorem ipsum
- Any UX changes necessary based on the logs, or feedback on user challenges, or VA challenges? yes/no 
- If yes, what: lorem ipsum

## Post-launch Questions 

1. How do the KPIs you gathered compare to your pre-launch definition(s) of "success"?
1. What qualitative feedback have you gathered from users or other stakeholders, if any?
1. Which of the assumptions you listed in your product outline were/were not validated? 
1. How might your product evolve now or in the future based on these results?
