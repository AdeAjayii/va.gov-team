# Chatbot Controlled Study Research Report (DRAFT)

#### Shane Strassberg and Rachel M. Murray, [research plan](https://github.com/department-of-veterans-affairs/va.gov-team/blob/master/products/virtual-agent/research/controlled-study/controlled-study-research-plan.md)

Please note this is a DRAFT version and is not yet finialized.


## Introduction

A well-developed and maintained virtual agent will help users solve problems and complete tasks on their own with little to no human involvement at any time of day. Ultimately, the problem we want to solve with our virtual agent is to help the Veteran self-serve and find information more easily as part of a larger Omnichannel strategy (led by VEO) to provide veterans with seamless access to information.  Because of the amount of content that is available and needs to be rewritten in conversational format for the chatbot, we want to target starting with smaller targeted content. A proof of value (POV) was created with a preliminary set of features and content; this report details the feedback on that product.


## Research Goals

We established a number of goals around the product and how people used it. There are many ways of understanding the success of a product, but for this research study we wanted to learn how the chatbot performed and how it was‘seen’ as being able to perform, since accuracy shapes how people trust a product. We also wanted to examine trust as tied to how personality can reinforce trust, and how all of these affected the larger brand relationship participants have with the VA support channels - the ultimate goal to understand if participants would use the chatbot again.

The goals of this study included:

- A: Accuracy: 
  - Understand how well the chatbot performed (answered questions accurately, understood what participants were trying to ask, and if participants had information needed to take the next step to complete a task
  - Understand if and how poor performance affected participants

- B: Trust:
  - Understand if people preferred to sign in (get answers to personal questions) or not to sign in (even if it means no personalized answers) and if they trusted the chatbot

- C: Personality:
  - Understand how participants felt about the voice and tone of the chatbot

- D: Product usage:
  - Understand how often participants speak with a VA help desk/contact center, reasons why and how a chatbot can reduce how often they call the VA
Understand if participants are willing to use the chatbot again

## Research Methodology

We launched with an unmoderated controlled study of 50 participants and were able to recruit 44 in total.  Participants were invited by Perigean to participate in the research study by email and have a number of research repositories that captured the data. 

- Participants were sent a link to the chatbot on Staging.va.gov and were able to engage with it. These are the ‘chatbot log transcripts’ which are reflected in the Metrics, and show the types of questions and interactions with the product.
- Participants were also sent a link to a questionnaire on Optimal Workshop. This included both quantitative questions where people were asked to rate their opinion from a preselected set of options, and qualitative questions where people were asked to provide written feedback.

## Hypotheses 

We identified a set of hypotheses which were in response to our goals, and identified data that would validate or refute them.

- A: Accuracy:
  - We will see a high number of questions that the bot has not yet been trained to answer. 
  - Veterans will expect that the chatbot cannot answer in-depth questions. 

- B: Trust:
  - More Veterans will attempt to find information that requires authentication (e.g., claims-status). 
  - Veterans who may be seeking anonymity (e.g., LGBTQ+, housing or food insecurity, mental-health crisis) may be more comfortable utilizing this tool over speaking with a human.

- C: Personality:
  - Veterans will want a more professional tone than a more ‘friendly’ personable tone.

- D: Product usage:

************TO UPDATE**********


## Participants

* Age: 
   * 18-24: 1
   * 25-34: 6
   * 35 - 44: 8
   * 45 - 54: 12
   * 55-64: 6
   * over 65: 8
   * participant no age stated: 3

* Education:
   * High school diploma or equivalent (ex: GED): 2
   * Some college (no degree): 16
   * Associate’s degree / trade certificate / vocational training: 6
   * Bachelors: 12
   * Graduate degree (Masters, PhD): 7
   * Blank: 1

* Gender:  
   * Women: 19
   * Men: 24
   * Transgender: 1

* Location:
   * AK, AR, CA, CO, FL, GA, HI, ID, IN, MD, MI, MN, NC, NY, PA, PR, SC, TN, TX, UT, VA, WY

* Race:
   * American Indian or Alaska Native: 1
   * Asian: 2
   * Black or African American: 3
   * Hispanic, Latino, or Spanish origin: 2
   * White or Caucasian: 31
   * Other: 1
   * Prefer not to answer / blank: 4

We divided participants into 10 segments, with a goal of 10 participants in each:

Group | Segment and number of participants
------------ | -------------
Veterans | Segment #1: 10 participants who are female Veterans across age brackets and conflicts/periods of service - Korean Conflict, Vietnam Era, Persian Gulf, Afghanistan, Iraq.  We were able to recruit 10 out of 10 of this segment.
Veterans | Segment #2: 10 participants who are male Veterans  across age brackets and conflicts/periods of service - Korean Conflict, Vietnam Era, Persian Gulf, Afghanistan, Iraq.  We were able to recruit 9 out of 10 of this segment.
Non-Veteran | Segment #3: 10 participants who are people close to Veterans - female caretakers, male caretakers or family members (i.e. dependents).  We were able to recruit 7 out of 10 of this segment.
Usage related | Segment #4: 10 participants who are new to va.gov (2 years or less)  We were able to recruit 3 out of 10 of this segment.
Usage related | Segment #5: 10 participants who are casual, infrequent users of va.gov (once a year).  We were able to recruit 8 out of 10 of this segment.
Usage related | Segment #6: 10 participants who are frequent users (dependent on claims etc. or who visit va.gov daily or weekly). We were able to recruit 7 out of 10 of this segment.
Location | Segment #7: 10 participants who live in urban centers. We were able to recruit 1 out of 10 of this segment.
Location | Segment #8: 10 participants who live in suburban or rural areas.  We were unable to recruit for this segment.
Marginalized populations | Segment #9: 10 participants who identify as LGBTQ+.  We were able to recruit 1 out of 10 of this segment.
Marginalized populations| Segment #10: 10 participants who are experiencing economic insecurity - (i.e. experiencing homelessness/housing insecure, food insecurity, either currently or previously in their time as a Veteran post-discharge from service). We were unable to recruit for this segment.

## Key Findings TO UPDATE

We identified 6 key findings:

A: Accuracy: 
- The chatbot was rated as being fairly accurate, although the level of accuracy left some participants feeling frustrated.

B: Trust:
- Participants understood the chatbot wasn’t a human but still felt they could trust it.
- Participation from marginalized populations was low and understanding their needs will be key to ensuring trust is built into the heart of the product.

C: Personality:
- Participants reacted positively to the personality of the chatbot in terms of voice and tone.

D: Product usage:
- Participants experience challenges with current VA customer support channels, and a VA chatbot was viewed as an opportunity to avoid those challenges.
- Overall users indicated a willingness to not only try the chatbot, but to use it again, and seemed excited about the product, how it could evolve and how it might help them and the VA.


## Details of Findings  TO UPDATE

TBD


A: Accuracy

- The chatbot was rated as being fairly accurate, although the level of accuracy left some participants feeling frustrated.
  - We hypothesized that we would see a high number of questions that the bot has not yet been trained to answer, and that Veterans will expect that the chatbot cannot answer in-depth questions. 
  - While we had the chat logs to monitor what questions were being asked, we also created a number of questions on accuracy including if the participants felt they had information to take the next step, if quality was satisfactory and if issues of poor performance affect participants.  In question 1 (“The chatbot answered my questions accurately”), participants provided feedback that 16% (7 out of 44) said the chatbot was accurate all the time, and 45% (20 out of 44) said most of the time it was accurate.  Many understood the limitations of the early version of this product in terms of what content was available and what functionality was available.  








## Metrics  TO UPDATE

one | two
------------ | -------------
one | two

## Results of Hypotheses

TBD


## Next Steps

Please see the final report for the Virtual Agent project for next steps, including actions to be taken for the product roadmap and additional research areas.

## Appendix

### Conversation guide
n/a

### Interview transcripts
n/a

### Pages and applications used
Pages tested: https://staging.va.gov/virtual-agent-study/ 


